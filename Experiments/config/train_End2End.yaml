# End2End Training Configuration
# This file contains all configuration parameters for training the End2End transformer model
# The configuration is organized into logical sections for data, training, model, callbacks, and logging

# Directory configuration for experiment outputs and temporary files
project_dir: ${pkgfile:fmukf}/../.. # Sneaky way to get the project directory
hydra_dir: ${project_dir}/Experiments/temp/training/end2end/${now:%Y-%m-%d}/${now:%H-%M-%S} # Directory for all temporary files
output_dir: ${project_dir}/Experiments/outputs

# Import other configuration files
defaults:
  - auth          # Authentication credentials for Comet ML
  - dataset       # Dataset configuration (train/val/test splits)
  - _self_        # This configuration file

# Hydra configuration for experiment management
hydra:
  job:
    chdir: true   # Change to experiment directory
  run:
    dir: "${hydra_dir}"  # Set run directory
  sweep:
    dir: "${hydra_dir}"  # Set sweep directory
    subdir: "${hydra.job.num}"  # Subdirectory for each job

# Data configuration - determines train/validation/test split (KEEP CONSISTENT WITH BENCHMARK)
data:
  h5_dataset_path: ${dataset.h5_dataset_path}  # Path to HDF5 dataset file
  seed: ${dataset.seed}                        # Random seed for reproducible train/val/test splits
  num_envs_val:  ${dataset.num_envs_val}       # Number of environments for validation
  num_envs_test: ${dataset.num_envs_test}      # Number of environments for testing
  batch_size: 256                              # (Initial) Batch size for training
  recompute_statistics: false                  # Whether to recompute dataset normalization statistics
  dtype: float32                               # Data type for tensors

# Training configuration
training:
  l_batch: 128                                 # Total sequence length for training
  max_epochs: 1000                             # Maximum number of training epochs (short for testing)
  check_val_every_n_epoch: 10                  # Validate every N epochs (frequent for testing)
  log_every_n_steps: 254                       # Log metrics every N steps
  enable_progress_bar: true                    # Show training progress bar
  gradient_clip_val: 1.0                       # Gradient clipping value for stability
  benchmark: true                              # Enable PyTorch benchmarking

# Masked Sensor congiruation
# Each configuration specifies which state features are observed (unmasked) during training
# Each feature those gets values gets perturbed with Gaussian noise with std value which for
# in every trajectory is sampled from the range of
# (model.min_std_multiplier * model.noise_stds_x[feature], model.max_std_multiplier * model.noise_stds_x[feature]).
# The values are specified below.
sensor_configs:
  - key: "ALL"
    y_vec_order: ["x", "y", "v", "u", "psi", "r", "phi", "p"] 
  - key: "GuessUV"
    y_vec_order: ["x", "y", "psi", "r", "phi", "p"]


# Model architecture and configuration
model:
  patch_size: 2                                # Size of patches for transformer input
  datatype: float32                            # Model data type
  h: 1.0                                       # Time step size in seconds
  normalize: true                              # Whether to normalize input data
  loss_type: Huber                             # Loss function type (Huber loss)
  
  # Transformer architecture parameters
  transformer_kwargs:
    embedding_dim: 32                          # Dimension of transformer embeddings (small for testing)
    num_layers: 8                              # Number of transformer layers
    num_heads: ${intop:${.embedding_dim},/,4}  # Number of attention heads
    transformer_hidden_dim: ${intop:4,*,${.embedding_dim}}  # Hidden dimension
    residual_dropout: 0.0                      # Dropout for residual connections
    residual_hidden_dim: ${intop:4,*,${.embedding_dim}}    # Residual hidden dimension
    attention_dropout: 0.01                    # Dropout for attention layers
    use_posenc: true                           # Whether to use positional encoding
    posenc_dropout: 0.01                       # Dropout for positional encoding
    posenc_max_len: ${training.l_batch}        # Maximum length for positional encoding
 
  # Noise configuration for model inputs
  # Small amount of noise added to the model's input (prior std ~ [min_std_multiplier * noise_stds_x, max_std_multiplier * noise_stds_x])
  min_std_multiplier: 0.3                      # Minimum standard deviation multiplier
  max_std_multiplier: 3.0                      # Maximum standard deviation multiplier
  noise_stds_x:
    x: 0.003                                   # X position noise
    y: 0.003                                   # Y position noise
    v: 0.1                                     # Velocity noise
    u: 0.1                                     # Forward velocity noise
    psi: 1.0                                   # Heading angle noise
    r: 0.1                                     # Yaw rate noise
    phi: 1.0                                   # Roll angle noise
    p: 0.1                                     # Roll rate noise
    delta: 0.0                                 # Rudder angle noise (no noise)
    n: 0.0                                     # Propeller speed noise (no noise)
  
  noise_stds_u:                                # Noise for input variables
    delta: 0.0                                 # Rudder angle noise (no noise)
    n: 0.0                                     # Propeller speed noise (no noise)
  
  # Optimization configuration
  optimization:
    optimizer: Adam                            # Optimizer type
    learning_rate: 0.001                       # Learning rate
    lr_schedule:                               # Learning rate schedule parameters
      warmup: 40                               # Warmup steps (short for testing)
      decay: 0.774                             # Decay factor
      step: 80                                 # Step size for decay

# Callbacks configuration for training monitoring and visualization
callbacks:
  # Gradient accumulation schedule for effective larger batch sizes
  grad_accumulation:
    schedule:
      1: 1                                     # Start with accumulation factor 1
      20: 4                                    # Increase to factor *4 at step 10 (short for testing)
      40: 8                                    # Increase to 8 at step 20

  # Model checkpointing configuration (saves the best model based on validation loss)
  checkpoint:
    use: true                                  # Whether to save checkpoints
    dirpath: '${hydra_dir}/ckpt'               # Directory for saving checkpoints
    filename: 'end2end__{experiment_name}-epoch{{epoch}}-{{val_loss:.10f}}'  # Checkpoint filename pattern
    monitor: val_loss                          # Metric to monitor for best model
    save_top_k: 3                              # Save top k best models
    verbose: true                              # Verbose checkpoint logging

  # Identity loss computation for sanity checking
  identity_loss:
    use: true                                  # Whether to compute identity loss

  # Visualization callbacks for monitoring training progress
  visABBpred:                                  # Raw prediction visualization
    every_n_epochs: 2                          # Visualize every N epochs (frequent for testing)
    at_train_end: true                         # Whether to visualize at training end
    backend: matplotlib                        # Visualization backend

# Logging configuration for experiment tracking
logger:  
  use_comet: false                             # Whether to use Comet ML for logging
  comet:
    api_key: ${auth.comet_logger.api_key}      # Comet ML API key
    workspace: ${auth.comet_logger.workspace}  # Comet ML workspace
    project_name: foo                          # Project name for experiment tracking
    save_dir: "${hydra_dir}/comet"             # Directory for Comet ML files
    tags:                                      # Tags for experiment organization
    - End2End                                  # Model type tag
    - Foo                                      # Project tag
    log_code_files:                            # Code files to log with experiment
    - ${pkgfile:fmukf.estimators.End2End}      # Model implementation file
    - ${pkgfile:fmukf.ML.models}               # Model implementation file